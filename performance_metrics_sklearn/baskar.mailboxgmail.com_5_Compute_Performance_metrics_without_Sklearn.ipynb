{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix :\n",
      " [[    0     0]\n",
      " [  100 10000]]\n",
      "\n",
      " \n",
      "Precision :\n",
      " 0.9900990099009901 \n",
      " Recall :\n",
      " 1.0 \n",
      " F1_Score :\n",
      "0.9950248756218906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10100/10100 [1:00:36<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : \n",
      " 0.48829900000000004 \n",
      "\n",
      "\n",
      "Accuracy_score :\n",
      "0.9900990099009901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Function to obtain derived class y\n",
    "def yclass(row):\n",
    "    if row['proba'] < 0.5:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "#Function to obtain derived class y for Auc score\n",
    "def auc_yclass(row,i):\n",
    "    if row['proba'] < i:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "#Function to return tpr_array,fpr_array for AUC calculation\n",
    "def auc_score(auc_threshold):\n",
    "    tpr_array=[]\n",
    "    fpr_array=[]\n",
    "    for i in tqdm(auc_threshold):\n",
    "        y_d1_class=[]\n",
    "        data['y_d1_class'] = data.apply(lambda row: auc_yclass(row,i), axis = 1)\n",
    "        y_a_class_lst=data['y'].tolist()\n",
    "        y_d1_class_lst=data['y_d1_class'].tolist()\n",
    "        y_class1_merged=()\n",
    "        y_class1_merged=tuple(zip( y_d1_class_lst,y_a_class_lst))\n",
    "        TN,FN,FP,TP=tpr_fpr(y_class1_merged)\n",
    "        #TPR,FPR,TNR,FNR\n",
    "        TPR=TP/(TP+FN)\n",
    "        TNR=TN/(TN+FP)\n",
    "        FPR=FP/(TN+FP)\n",
    "        FNR=FN/(FN+TP)\n",
    "        tpr_array.append(TPR)\n",
    "        fpr_array.append(FPR)\n",
    "    return tpr_array,fpr_array\n",
    "\n",
    "#function to return TN,FN,FP,TP values\n",
    "def tpr_fpr(y_class_merged):\n",
    "    TN,FP,FN,TP=0,0,0,0\n",
    "    for i in range(len(y_class_merged)):\n",
    "        if  y_class_merged[i][0]==0.0 and y_class_merged[i][1]==0.0:\n",
    "            TN=TN+1\n",
    "        elif y_class_merged[i][0]==0.0 and y_class_merged[i][1]==1.0:\n",
    "            FN=FN+1\n",
    "        elif y_class_merged[i][0]==1.0 and y_class_merged[i][1]==0.0:\n",
    "            FP=FP+1\n",
    "        else:\n",
    "            TP=TP+1\n",
    "    return TN,FN,FP,TP\n",
    "        \n",
    "    \n",
    "#load 5_a.csv dataset\n",
    "data=pd.read_csv('5_a.csv')\n",
    "\n",
    "#derive class labels from probability score\n",
    "data['y_d_class'] = data.apply(lambda row: yclass(row), axis = 1) \n",
    "\n",
    "y_a_class_lst=[]\n",
    "y_d_class_lst=[]\n",
    "y_a_class_lst=data['y'].tolist()\n",
    "y_d_class_lst=data['y_d_class'].tolist()\n",
    "y_class_merged=tuple(zip( y_d_class_lst,y_a_class_lst))\n",
    "\n",
    "#function call to calculate TN,FN,TP,FP\n",
    "TN,FN,FP,TP=tpr_fpr(y_class_merged)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix_lst=[]\n",
    "confusion_matrix_lst.append(TN)\n",
    "confusion_matrix_lst.append(FN)\n",
    "confusion_matrix_lst.append(FP)\n",
    "confusion_matrix_lst.append(TP)\n",
    "cm_array=np.asarray(confusion_matrix_lst)\n",
    "confusion_matrix = cm_array.reshape(2, 2)\n",
    "print(\"confusion_matrix :\\n {}\\n\\n \" .format(confusion_matrix))\n",
    "\n",
    "#TPR,FPR,TNR,FNR\n",
    "TPR=TP/(TP+FN)\n",
    "TNR=TN/(TN+FP)\n",
    "FPR=FP/(TN+FP)\n",
    "FNR=FN/(FN+TP)\n",
    "\n",
    "#F1_score calculation\n",
    "precision=TP/(TP+FP)\n",
    "recall=TPR\n",
    "F1_score=2*((precision*recall)/(precision+recall))\n",
    "print('Precision :\\n {} \\n Recall :\\n {} \\n F1_Score :\\n{}\\n'.format(precision,recall,F1_score))\n",
    "\n",
    "#AUC score calculation\n",
    "auc_threshold=[]\n",
    "auc_threshold=data['proba'].tolist()\n",
    "auc_threshold.sort(reverse=True)\n",
    "#AUC_score function call\n",
    "tpr_array,fpr_array=auc_score(auc_threshold)\n",
    "    \n",
    "Auc_score=np.trapz(tpr_array, fpr_array)\n",
    "print('AUC Score : \\n {} \\n\\n' .format(Auc_score))\n",
    "\n",
    "#Accuracy score\n",
    "Accuracy_score=(int(confusion_matrix[0][0])+int(confusion_matrix[1][1])) / int(len(data))\n",
    "print('Accuracy_score :\\n{}\\n\\n'.format(Accuracy_score))\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix :\n",
      " [[9761   45]\n",
      " [ 239   55]]\n",
      "\n",
      " \n",
      "Precision :\n",
      " 0.1870748299319728 \n",
      " Recall :\n",
      " 0.55 \n",
      " F1_Score :\n",
      "0.2791878172588833\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10100/10100 [1:00:54<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : \n",
      " 0.9377570000000001 \n",
      "\n",
      "\n",
      "Accuracy_score :\n",
      "0.9718811881188119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Function to obtain derived class y\n",
    "def yclass(row):\n",
    "    if row['proba'] < 0.5:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "#Function to obtain derived class y for Auc score\n",
    "def auc_yclass(row,i):\n",
    "    if row['proba'] < i:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "#Function to return tpr_array,fpr_array for AUC calculation\n",
    "def auc_score(auc_threshold):\n",
    "    tpr_array=[]\n",
    "    fpr_array=[]\n",
    "    for i in tqdm(auc_threshold):\n",
    "        y_d1_class=[]\n",
    "        data['y_d1_class'] = data.apply(lambda row: auc_yclass(row,i), axis = 1)\n",
    "        y_a_class_lst=data['y'].tolist()\n",
    "        y_d1_class_lst=data['y_d1_class'].tolist()\n",
    "        y_class1_merged=()\n",
    "        y_class1_merged=tuple(zip( y_d1_class_lst,y_a_class_lst))\n",
    "        TN,FN,FP,TP=tpr_fpr(y_class1_merged)\n",
    "        #TPR,FPR,TNR,FNR\n",
    "        TPR=TP/(TP+FN)\n",
    "        TNR=TN/(TN+FP)\n",
    "        FPR=FP/(TN+FP)\n",
    "        FNR=FN/(FN+TP)\n",
    "        tpr_array.append(TPR)\n",
    "        fpr_array.append(FPR)\n",
    "    return tpr_array,fpr_array\n",
    "\n",
    "#function to return TN,FN,FP,TP values\n",
    "def tpr_fpr(y_class_merged):\n",
    "    TN,FP,FN,TP=0,0,0,0\n",
    "    for i in range(len(y_class_merged)):\n",
    "        if  y_class_merged[i][0]==0.0 and y_class_merged[i][1]==0.0:\n",
    "            TN=TN+1\n",
    "        elif y_class_merged[i][0]==0.0 and y_class_merged[i][1]==1.0:\n",
    "            FN=FN+1\n",
    "        elif y_class_merged[i][0]==1.0 and y_class_merged[i][1]==0.0:\n",
    "            FP=FP+1\n",
    "        else:\n",
    "            TP=TP+1\n",
    "    return TN,FN,FP,TP\n",
    "        \n",
    "    \n",
    "#load 5_a.csv dataset\n",
    "data=pd.read_csv('5_b.csv')\n",
    "\n",
    "#derive class labels from probability score\n",
    "data['y_d_class'] = data.apply(lambda row: yclass(row), axis = 1) \n",
    "\n",
    "y_a_class_lst=[]\n",
    "y_d_class_lst=[]\n",
    "y_a_class_lst=data['y'].tolist()\n",
    "y_d_class_lst=data['y_d_class'].tolist()\n",
    "y_class_merged=tuple(zip( y_d_class_lst,y_a_class_lst))\n",
    "\n",
    "#function call to calculate TN,FN,TP,FP\n",
    "TN,FN,FP,TP=tpr_fpr(y_class_merged)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix_lst=[]\n",
    "confusion_matrix_lst.append(TN)\n",
    "confusion_matrix_lst.append(FN)\n",
    "confusion_matrix_lst.append(FP)\n",
    "confusion_matrix_lst.append(TP)\n",
    "cm_array=np.asarray(confusion_matrix_lst)\n",
    "confusion_matrix = cm_array.reshape(2, 2)\n",
    "print(\"confusion_matrix :\\n {}\\n\\n \" .format(confusion_matrix))\n",
    "\n",
    "#TPR,FPR,TNR,FNR\n",
    "TPR=TP/(TP+FN)\n",
    "TNR=TN/(TN+FP)\n",
    "FPR=FP/(TN+FP)\n",
    "FNR=FN/(FN+TP)\n",
    "\n",
    "#F1_score calculation\n",
    "precision=TP/(TP+FP)\n",
    "recall=TPR\n",
    "F1_score=2*((precision*recall)/(precision+recall))\n",
    "print('Precision :\\n {} \\n Recall :\\n {} \\n F1_Score :\\n{}\\n'.format(precision,recall,F1_score))\n",
    "\n",
    "#AUC score calculation\n",
    "auc_threshold=[]\n",
    "auc_threshold=data['proba'].tolist()\n",
    "auc_threshold.sort(reverse=True)\n",
    "#AUC_score function call\n",
    "tpr_array,fpr_array=auc_score(auc_threshold)\n",
    "    \n",
    "Auc_score=np.trapz(tpr_array, fpr_array)\n",
    "print('AUC Score : \\n {} \\n\\n' .format(Auc_score))\n",
    "\n",
    "#Accuracy score\n",
    "Accuracy_score=(int(confusion_matrix[0][0])+int(confusion_matrix[1][1])) / int(len(data))\n",
    "print('Accuracy_score :\\n{}\\n\\n'.format(Accuracy_score))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2852/2852 [05:13<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold probabilty : 0.2300390278970873\t Metric_A value : 141000\n"
     ]
    }
   ],
   "source": [
    "      \n",
    "#Function to obtain derived class y for Auc score\n",
    "def auc_yclass(row,i):\n",
    "    if row['prob'] < i:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "#Function to return tpr_array,fpr_array for AUC calculation\n",
    "def auc_score(auc_threshold):\n",
    "    tpr_array=[]\n",
    "    fpr_array=[]\n",
    "    metric_a_lst=[]\n",
    "    for i in tqdm(auc_threshold):\n",
    "        y_d1_class=[]\n",
    "        data['y_d1_class'] = data.apply(lambda row: auc_yclass(row,i), axis = 1)\n",
    "        y_a_class_lst=data['y'].tolist()\n",
    "        y_d1_class_lst=data['y_d1_class'].tolist()\n",
    "        y_class1_merged=()\n",
    "        y_class1_merged=tuple(zip( y_d1_class_lst,y_a_class_lst))\n",
    "        TN,FN,FP,TP=tpr_fpr(y_class1_merged)\n",
    "        metric_a_lst.append(500*FN+100*FP)\n",
    "        \n",
    "        #TPR,FPR,TNR,FNR\n",
    "        TPR=TP/(TP+FN)\n",
    "        TNR=TN/(TN+FP)\n",
    "        FPR=FP/(TN+FP)\n",
    "        FNR=FN/(FN+TP)\n",
    "        tpr_array.append(TPR)\n",
    "        fpr_array.append(FPR)\n",
    "    return tpr_array,fpr_array,metric_a_lst\n",
    "\n",
    "#function to return TN,FN,FP,TP values\n",
    "def tpr_fpr(y_class_merged):\n",
    "    TN,FP,FN,TP=0,0,0,0\n",
    "    for i in range(len(y_class_merged)):\n",
    "        if  y_class_merged[i][0]==0.0 and y_class_merged[i][1]==0.0:\n",
    "            TN=TN+1\n",
    "        elif y_class_merged[i][0]==0.0 and y_class_merged[i][1]==1.0:\n",
    "            FN=FN+1\n",
    "        elif y_class_merged[i][0]==1.0 and y_class_merged[i][1]==0.0:\n",
    "            FP=FP+1\n",
    "        else:\n",
    "            TP=TP+1\n",
    "    return TN,FN,FP,TP\n",
    "        \n",
    "    \n",
    "#load 5_a.csv dataset\n",
    "data=pd.read_csv('5_c.csv')\n",
    "\n",
    "#AUC score calculation\n",
    "auc_threshold=[]\n",
    "auc_threshold=data['prob'].tolist()\n",
    "auc_threshold.sort(reverse=True)\n",
    "#AUC_score function call\n",
    "tpr_array,fpr_array,metric_a_lst=auc_score(auc_threshold)\n",
    "\n",
    "#Best threshold prob value\n",
    "metric_a_dict = dict(zip(auc_threshold, metric_a_lst)) \n",
    "\n",
    "metric_a_lst_asc=sorted(metric_a_dict,key=metric_a_dict.get)[:1]\n",
    "for i in metric_a_lst_asc:\n",
    "    print('Best threshold probabilty : {0}\\t Metric_A value : {1}'.format(i,metric_a_dict[i]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : \n",
      " 177.16569974554707 \n",
      " \n",
      "MAPE : \n",
      " 12.91202994009687 \n",
      " \n",
      "R^2 : \n",
      " 0.9563582786990964 \n"
     ]
    }
   ],
   "source": [
    "#load 5_a.csv dataset\n",
    "data=pd.read_csv('5_d.csv')\n",
    "\n",
    "#Mean square error,average of square of y-pred\n",
    "y_lst=[]\n",
    "y_pred_lst=[]\n",
    "y_lst=data['y'].tolist()\n",
    "y_pred_lst=data['pred'].tolist()\n",
    "\n",
    "error_sum=0\n",
    "y_sum=0\n",
    "for i in range(len(y_lst)):\n",
    "    error_sum= error_sum + ((y_lst[i]-y_pred_lst[i])**2)\n",
    "\n",
    "mean_sq_err=error_sum/len(y_lst)\n",
    "print('Mean Squared Error : \\n {} \\n '.format(mean_sq_err))\n",
    "\n",
    "#MAPE,Mean absolute percentage error,sum of abs value of error(y-pred) divided by sum of values of y\n",
    "error_sum=0\n",
    "y_sum=0\n",
    "for i in range(len(y_lst)):\n",
    "    error_sum= error_sum +abs(y_pred_lst[i]-y_lst[i])\n",
    "    y_sum=y_sum+y_lst[i]\n",
    "MAPE=(error_sum/y_sum)*100\n",
    "print('MAPE : \\n {} \\n '.format(MAPE))\n",
    "\n",
    "#R squared,1-(ss_res/ss_total)\n",
    "ss_total=0\n",
    "ss_res=0\n",
    "for i in range(len(y_lst)):\n",
    "    ss_total=ss_total+((y_lst[i]-y_avg)**2)\n",
    "    ss_res=ss_res+((y_lst[i]-y_pred_lst[i])**2)\n",
    "\n",
    "R_2=1-(ss_res/ss_total)\n",
    "print('R^2 : \\n {} '.format(R_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
